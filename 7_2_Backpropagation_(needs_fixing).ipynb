{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkMc+6M22zox8OZsZcCZdF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djdumpling/Machine-Learning-Prince-/blob/main/7_2_Backpropagation_(needs_fixing).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5je3udWBaG8L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "# Number of layers\n",
        "K=5\n",
        "# Number of neurons per layer\n",
        "D=6\n",
        "# Input layer\n",
        "D_i = 1\n",
        "# Output layer\n",
        "D_o = 1\n",
        "\n",
        "# Make empty lists\n",
        "all_weights = [None] * (K+1)\n",
        "all_biases = [None] * (K+1)\n",
        "\n",
        "# Create input and output layers\n",
        "all_weights[0] = np.random.normal(size=(D, D_i))\n",
        "all_weights[-1] = np.random.normal(size=(D_o, D))\n",
        "all_biases[0] = np.random.normal(size =(D,1))\n",
        "all_biases[-1]= np.random.normal(size =(D_o,1))\n",
        "\n",
        "# Create intermediate layers\n",
        "for k in range(1, K):\n",
        "    all_weights[k] = np.random.normal(size=(D, D))\n",
        "    all_biases[k] = np.random.normal(size=(D, 1))"
      ],
      "metadata": {
        "id": "Fwws1j9pabLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU(preactivation):\n",
        "  activation = preactivation.clip(0.0)\n",
        "  return activation"
      ],
      "metadata": {
        "id": "zd0m-JR279Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_network_output(net_input, all_weights, all_biases):\n",
        "  K = len(all_weights) - 1\n",
        "\n",
        "  all_f = [None] * (K+1)\n",
        "  all_h = [None] * (K+1)\n",
        "\n",
        "  all_h[0] = net_input\n",
        "\n",
        "  for layer in range(K):\n",
        "    all_f[layer] = np.matmul(all_weights[layer], all_h[layer]) + all_biases[layer]\n",
        "    all_h[layer+1] = ReLU(all_f[layer])\n",
        "\n",
        "  all_f[K] = np.matmul(all_weights[K], all_h[K]) + all_biases[K]\n",
        "\n",
        "  net_output = all_f[K]\n",
        "\n",
        "  return net_output, all_f, all_h"
      ],
      "metadata": {
        "id": "3OzTW9A-8x55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_input = np.ones((D_i,1)) * 1.2\n",
        "net_output, all_f, all_h = compute_network_output(net_input,all_weights, all_biases)\n",
        "print(\"True output = %3.3f, Your answer = %3.3f\"%(1.907, net_output[0,0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2V3gfQs-yk1",
        "outputId": "31abd4a7-d151-4c71-ed4d-3872291fad74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True output = 1.907, Your answer = 1.907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def least_squares_loss(net_output, y):\n",
        "  return np.sum((net_output-y) * (net_output-y))\n",
        "\n",
        "def d_loss_d_output(net_output, y):\n",
        "  return 2*(net_output-y)\n",
        "\n",
        "y = np.ones((D_o,1)) * 20.0\n",
        "loss = least_squares_loss(net_output, y)\n",
        "print(\"y = %3.3f Loss = %3.3f\"%(y, loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDGSdxho_5fU",
        "outputId": "1dadae51-23d5-4e91-c491-6a44dd77f442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y = 20.000 Loss = 327.371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b8e329e9b0a5>:9: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"y = %3.3f Loss = %3.3f\"%(y, loss))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def indicator_function(x):\n",
        "  x_in = np.array(x)\n",
        "  x_in[x_in>=0] = 1\n",
        "  x_in[x_in<0] = 0\n",
        "  return x_in"
      ],
      "metadata": {
        "id": "IAQZuV40CgX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass(all_weights, all_biases, all_f, all_h, y):\n",
        "\n",
        "  K = len(all_weights) - 1\n",
        "\n",
        "  # store derivative dl_dweights and dl_dbiases in lists as well\n",
        "  all_dl_dweights = [None] * (K+1)\n",
        "  all_dl_dbiases = [None] * (K+1)\n",
        "\n",
        "  # store derivatives of the loss w.r.t activation and preactivation in lists\n",
        "  all_dl_df = [None] * (K+1)\n",
        "  all_dl_dh = [None] * (K+1)\n",
        "\n",
        "  #sticking with the convention that all_h[0] is the net input and all_f[-1] is the net output\n",
        "\n",
        "  all_dl_df[K] = np.array(d_loss_d_output(all_f[K], y))\n",
        "\n",
        "  for layer in range(K, -1, -1):\n",
        "    # TODO Calculate the derivatives of the loss with respect to the biases at layer from all_dl_df[layer]. (eq 7.21)\n",
        "    # NOTE!  To take a copy of matrix X, use Z=np.array(X)\n",
        "    all_dl_dbiases[layer] = np.array(all_dl_df[layer])\n",
        "\n",
        "    # TODO Calculate the derivatives of the loss with respect to the weights at layer from all_dl_df[layer] and all_h[layer] (eq 7.22)\n",
        "    all_dl_dweights[layer] = np.matmul(all_dl_df[layer], all_h[layer].transpose())\n",
        "\n",
        "    # TODO: calculate the derivatives of the loss with respect to the activations from weight and derivatives of next preactivations (second part of last line of eq 7.24)\n",
        "    all_dl_dh[layer] = np.matmul(all_weights[layer].transpose(), all_dl_df[layer])\n",
        "\n",
        "    if layer > 0:\n",
        "      # TODO Calculate the derivatives of the loss with respect to the pre-activation f (use derivative of ReLu function, first part of last line of eq. 7.24)\n",
        "      all_dl_df[layer-1] = indicator_function(all_f[layer]) * all_dl_dh[layer]\n",
        "\n",
        "  return all_dl_dweights, all_dl_dbiases"
      ],
      "metadata": {
        "id": "YlnjToE5C8wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_dl_dweights, all_dl_dbiases = backward_pass(all_weights, all_biases, all_f, all_h, y)"
      ],
      "metadata": {
        "id": "Fj7d6j9yms2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(precision=3)\n",
        "# Make space for derivatives computed by finite differences\n",
        "all_dl_dweights_fd = [None] * (K+1)\n",
        "all_dl_dbiases_fd = [None] * (K+1)\n",
        "\n",
        "# Let's test if we have the derivatives right using finite differences\n",
        "delta_fd = 0.000001\n",
        "\n",
        "# Test the dervatives of the bias vectors\n",
        "for layer in range(K):\n",
        "  dl_dbias  = np.zeros_like(all_dl_dbiases[layer])\n",
        "  # For every element in the bias\n",
        "  for row in range(all_biases[layer].shape[0]):\n",
        "    # Take copy of biases  We'll change one element each time\n",
        "    all_biases_copy = [np.array(x) for x in all_biases]\n",
        "    all_biases_copy[layer][row] += delta_fd\n",
        "    network_output_1, *_ = compute_network_output(net_input, all_weights, all_biases_copy)\n",
        "    network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n",
        "    dl_dbias[row] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n",
        "  all_dl_dbiases_fd[layer] = np.array(dl_dbias)\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Bias %d, derivatives from backprop:\"%(layer))\n",
        "  print(all_dl_dbiases[layer])\n",
        "  print(\"Bias %d, derivatives from finite differences\"%(layer))\n",
        "  print(all_dl_dbiases_fd[layer])\n",
        "  if np.allclose(all_dl_dbiases_fd[layer],all_dl_dbiases[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n",
        "    print(\"Success!  Derivatives match.\")\n",
        "  else:\n",
        "    print(\"Failure!  Derivatives different.\")\n",
        "\n",
        "# Test the derivatives of the weights matrices\n",
        "for layer in range(K):\n",
        "  dl_dweight  = np.zeros_like(all_dl_dweights[layer])\n",
        "  # For every element in the bias\n",
        "  for row in range(all_weights[layer].shape[0]):\n",
        "    for col in range(all_weights[layer].shape[1]):\n",
        "      # Take copy of biases  We'll change one element each time\n",
        "      all_weights_copy = [np.array(x) for x in all_weights]\n",
        "      all_weights_copy[layer][row][col] += delta_fd\n",
        "      network_output_1, *_ = compute_network_output(net_input, all_weights_copy, all_biases)\n",
        "      network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n",
        "      dl_dweight[row][col] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n",
        "  all_dl_dweights_fd[layer] = np.array(dl_dweight)\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Weight %d, derivatives from backprop:\"%(layer))\n",
        "  print(all_dl_dweights[layer])\n",
        "  print(\"Weight %d, derivatives from finite differences\"%(layer))\n",
        "  print(all_dl_dweights_fd[layer])\n",
        "  if np.allclose(all_dl_dweights_fd[layer],all_dl_dweights[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n",
        "    print(\"Success!  Derivatives match.\")\n",
        "  else:\n",
        "    print(\"Failure!  Derivatives different.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5fo9xEemran",
        "outputId": "eb2f48d6-474d-499c-8efa-29d173ec0113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------\n",
            "Bias 0, derivatives from backprop:\n",
            "[[ -0.   ]\n",
            " [ 24.469]\n",
            " [  0.   ]\n",
            " [  0.   ]\n",
            " [-30.963]\n",
            " [ -0.   ]]\n",
            "Bias 0, derivatives from finite differences\n",
            "[[ -4.486]\n",
            " [  4.947]\n",
            " [  6.812]\n",
            " [ -3.883]\n",
            " [-24.935]\n",
            " [  0.   ]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Bias 1, derivatives from backprop:\n",
            "[[  0.   ]\n",
            " [  0.   ]\n",
            " [-44.856]\n",
            " [ -0.   ]\n",
            " [ -2.557]\n",
            " [ 18.601]]\n",
            "Bias 1, derivatives from finite differences\n",
            "[[  0.   ]\n",
            " [-11.297]\n",
            " [  0.   ]\n",
            " [  0.   ]\n",
            " [-10.722]\n",
            " [  0.   ]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Bias 2, derivatives from backprop:\n",
            "[[  0.   ]\n",
            " [ 30.165]\n",
            " [ 18.282]\n",
            " [ -0.   ]\n",
            " [ 24.196]\n",
            " [-30.257]]\n",
            "Bias 2, derivatives from finite differences\n",
            "[[ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.938]\n",
            " [ 0.   ]\n",
            " [-9.993]\n",
            " [ 0.508]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Bias 3, derivatives from backprop:\n",
            "[[-0.   ]\n",
            " [ 0.   ]\n",
            " [-0.   ]\n",
            " [ 0.   ]\n",
            " [44.869]\n",
            " [ 0.   ]]\n",
            "Bias 3, derivatives from finite differences\n",
            "[[ 0.   ]\n",
            " [-4.8  ]\n",
            " [-1.661]\n",
            " [ 0.   ]\n",
            " [ 3.393]\n",
            " [ 5.391]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Bias 4, derivatives from backprop:\n",
            "[[-34.381]\n",
            " [  5.477]\n",
            " [  3.735]\n",
            " [-14.858]\n",
            " [ -5.212]\n",
            " [-52.625]]\n",
            "Bias 4, derivatives from finite differences\n",
            "[[ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [-5.212]\n",
            " [ 0.   ]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Weight 0, derivatives from backprop:\n",
            "[[  0.   ]\n",
            " [ 29.363]\n",
            " [  0.   ]\n",
            " [  0.   ]\n",
            " [-37.156]\n",
            " [  0.   ]]\n",
            "Weight 0, derivatives from finite differences\n",
            "[[ -5.383]\n",
            " [  5.937]\n",
            " [  8.174]\n",
            " [ -4.66 ]\n",
            " [-29.922]\n",
            " [  0.   ]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Weight 1, derivatives from backprop:\n",
            "[[   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [-129.092  -26.997  -72.593 -135.589 -167.544    0.   ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [  -7.36    -1.539   -4.139   -7.73    -9.552    0.   ]\n",
            " [  53.533   11.196   30.104   56.228   69.479    0.   ]]\n",
            "Weight 1, derivatives from finite differences\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [-32.511  -6.799 -18.282 -34.148 -42.196   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [-30.856  -6.453 -17.352 -32.409 -40.047   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Weight 2, derivatives from backprop:\n",
            "[[   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.     172.758    0.       0.     101.139    0.   ]\n",
            " [   0.     104.705    0.       0.      61.298    0.   ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.     138.57     0.       0.      81.124    0.   ]\n",
            " [   0.    -173.282    0.       0.    -101.446    0.   ]]\n",
            "Weight 2, derivatives from finite differences\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      5.371   0.      0.      3.145   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.    -57.233   0.      0.    -33.506   0.   ]\n",
            " [  0.      2.907   0.      0.      1.702   0.   ]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Weight 3, derivatives from backprop:\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.     34.344   0.    401.067 102.811]\n",
            " [  0.      0.      0.      0.      0.      0.   ]]\n",
            "Weight 3, derivatives from finite differences\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.     -3.674   0.    -42.905 -10.998]\n",
            " [  0.      0.     -1.272   0.    -14.85   -3.807]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      2.597   0.     30.333   7.776]\n",
            " [  0.      0.      4.126   0.     48.188  12.353]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Weight 4, derivatives from backprop:\n",
            "[[   0.    -538.454 -324.091    0.    -145.153  -66.918]\n",
            " [   0.      85.78    51.63     0.      23.124   10.661]\n",
            " [   0.      58.498   35.21     0.      15.77     7.27 ]\n",
            " [   0.    -232.703 -140.062    0.     -62.73   -28.92 ]\n",
            " [   0.     -81.635  -49.136    0.     -22.007  -10.146]\n",
            " [   0.    -824.197 -496.076    0.    -222.181 -102.43 ]]\n",
            "Weight 4, derivatives from finite differences\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.    -81.635 -49.136   0.    -22.007 -10.146]\n",
            " [  0.      0.      0.      0.      0.      0.   ]]\n",
            "Failure!  Derivatives different.\n"
          ]
        }
      ]
    }
  ]
}